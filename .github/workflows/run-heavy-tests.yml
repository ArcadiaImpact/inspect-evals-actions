---
name: Heavy Tests

"on":
  workflow_dispatch:
    inputs:
      inspect_evals_ref:
        description: "inspect_evals branch/tag/commit to checkout"
        required: false
        default: ""
        type: string
  schedule:
    - cron: "0 2 * * *"  # Nightly at 02:00 UTC

jobs:
  heavy-tests:
    name: Heavy tests (full suite)
    runs-on: ubuntu-latest-8-cores
    timeout-minutes: 180
    env:
      HF_TOKEN: ${{ secrets.HF_ACCESS_TOKEN }}
      RUN_SLOW_TESTS: yes
      RUN_DATASET_DOWNLOAD_TESTS: yes
      TRACE_DOCKER_BUILDS: yes
      SLOW_TEST_THRESHOLD: 10.0  # seconds
    steps:
      # Critical: Setup modern Docker build system
      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          version: latest
          driver-opts: |
            network=host
          buildkitd-flags: |
            --allow-insecure-entitlement security.insecure
            --allow-insecure-entitlement network.host

      - name: Download last tested inspect_evals SHA artifact from previous run
        id: download_artifact
        run: |
          # Find the most recent successful run that actually has the artifact
          echo "Searching for a successful run with the 'last-inspect-evals-sha' artifact..."

          # Get successful runs and check each one for the artifact
          RUN_ID=""
          for run_id in $(gh api repos/${{ github.repository }}/actions/workflows/run-heavy-tests.yml/runs \
            --jq '.workflow_runs[] | select(.conclusion == "success") | .id' | head -10); do

            echo "Checking run $run_id for artifacts..."
            artifact_count=$(gh api repos/${{ github.repository }}/actions/runs/$run_id/artifacts \
              --jq '.artifacts[] | select(.name == "last-inspect-evals-sha") | .name' | wc -l)

            if [ "$artifact_count" -gt 0 ]; then
              RUN_ID=$run_id
              echo "Found run with artifact: $RUN_ID"
              break
            fi
          done

          if [ -n "$RUN_ID" ]; then
            # Create directory for artifact
            mkdir -p .last-inspect-evals-sha

            # Download the artifact from the actions repository (where this workflow runs)
            if gh run download $RUN_ID --name last-inspect-evals-sha --dir .last-inspect-evals-sha --repo ${{ github.repository }}; then
              echo "Successfully downloaded artifact from run $RUN_ID"

              # Read the SHA immediately and store it as output (before checkout wipes it)
              if [ -f ".last-inspect-evals-sha/last_sha.txt" ]; then
                last_sha=$(cat ".last-inspect-evals-sha/last_sha.txt")
                echo "last_sha=$last_sha" >> $GITHUB_OUTPUT
                echo "Found previously tested commit: $last_sha (from run $RUN_ID)"
              else
                echo "last_sha=" >> $GITHUB_OUTPUT
                echo "Downloaded artifact but could not find last_sha.txt file"
              fi
            else
              echo "last_sha=" >> $GITHUB_OUTPUT
              echo "Artifact download failed for run $RUN_ID"
            fi
          else
            echo "last_sha=" >> $GITHUB_OUTPUT
            echo "No previous successful runs with artifacts found"
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Checkout inspect_evals repository
        uses: actions/checkout@v5
        with:
          repository: UKGovernmentBEIS/inspect_evals
          ref: ${{ inputs.inspect_evals_ref || '' }}

      - name: Get inspect_evals commit info
        id: inspect_commit
        run: |
          echo "sha=$(git rev-parse HEAD)" >> $GITHUB_OUTPUT
          echo "short_sha=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "ref=${{ inputs.inspect_evals_ref || 'main' }}" >> $GITHUB_OUTPUT

      - name: Check if commit was already tested
        id: check_commit
        run: |
          current_sha="${{ steps.inspect_commit.outputs.sha }}"
          last_sha="${{ steps.download_artifact.outputs.last_sha }}"

          echo "Current commit: $current_sha"

          if [ -n "$last_sha" ]; then
            echo "Found previously tested commit: $last_sha"
            if [ "$current_sha" = "$last_sha" ]; then
              echo "skip_tests=true" >> $GITHUB_OUTPUT
              echo "Commit $current_sha was already tested successfully, skipping"
              exit 0
            else
              echo "Current commit $current_sha differs from last tested commit $last_sha"
            fi
          else
            echo "No previous artifact found or could not read commit SHA"
          fi
          echo "skip_tests=false" >> $GITHUB_OUTPUT
          echo "Proceeding with tests for commit $current_sha"

      - name: Skip notification
        if: github.event_name == 'schedule' && (steps.check_commit.outputs.skip_tests == 'true')
        run: |
          echo "ðŸ”„ Skipping tests - no changes detected in inspect_evals repository"
          echo "Last tested commit: ${{ steps.inspect_commit.outputs.sha }}"

      - name: Set up Python 3.11
        if: steps.check_commit.outputs.skip_tests != 'true'
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Install uv
        if: steps.check_commit.outputs.skip_tests != 'true'
        uses: astral-sh/setup-uv@v6

      - name: Cache uv downloads
        if: steps.check_commit.outputs.skip_tests != 'true'
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
          key: ${{ runner.os }}-uv-3.11-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-3.11-
            ${{ runner.os }}-uv-

      - name: Install dependencies
        if: steps.check_commit.outputs.skip_tests != 'true'
        run: |
          uv sync --frozen --extra test_py311_or_lower

      - name: Run full test suite with report log
        if: steps.check_commit.outputs.skip_tests != 'true'
        run: |
          uv run pytest -rA --color=yes --cov=inspect_evals \
            --durations=0 \
            --report-log=pytest-report.jsonl

      - name: Detect tests >=10s that are missing @pytest.mark.slow
        if: steps.check_commit.outputs.skip_tests != 'true'
        run: |
          uv run python - <<'PY'
          import json, sys, os
          THRESHOLD = float(os.environ.get('SLOW_TEST_THRESHOLD', '10.0'))
          report = 'pytest-report.jsonl'
          offenders = []
          with open(report, 'r') as f:
              for line in f:
                  try:
                      obj = json.loads(line)
                  except Exception:
                      continue
                  if obj.get('$report_type') != 'TestReport':
                      continue
                  if obj.get('when') != 'call':
                      continue
                  # Skip skipped tests
                  if obj.get('outcome') == 'skipped':
                      continue
                  dur = float(obj.get('duration') or 0.0)
                  kw = obj.get('keywords') or {}
                  # keywords is a dict of names -> True
                  is_slow = 'slow' in kw
                  if dur >= THRESHOLD and not is_slow:
                      offenders.append((dur, obj.get('nodeid', '<unknown>')))
          offenders.sort(reverse=True)
          with open('unmarked_slow_tests.txt', 'w') as out:
              for dur, nodeid in offenders:
                  out.write(f"{dur:.2f}s\t{nodeid}\n")
          if offenders:
              print('\nFound tests taking >= 10s without @pytest.mark.slow:')
              for dur, nodeid in offenders:
                  print(f" - {dur:.2f}s  {nodeid}")
              sys.exit(1)
          else:
              print('No unmarked slow tests found (>= 10s).')
          PY

      - name: Detect tests using Docker that are missing @pytest.mark.docker
        if: steps.check_commit.outputs.skip_tests != 'true'
        run: |
          uv run python - <<'PY'
          import json, sys, os
          build_file = '.docker_build_tests.txt'
          compose_file = '.docker_compose_tests.txt'
          used = set()
          for path in (build_file, compose_file):
              if os.path.exists(path):
                  with open(path, 'r') as f:
                      for line in f:
                          nodeid = line.strip()
                          if nodeid:
                              used.add(nodeid)
          if not used:
              print('No docker usage detected (or trace files empty); skipping docker marker check.')
              sys.exit(0)
          # Map nodeid -> keywords from pytest report
          report = 'pytest-report.jsonl'
          keywords = {}
          try:
              with open(report, 'r') as f:
                  for line in f:
                      try:
                          obj = json.loads(line)
                      except Exception:
                          continue
                      if obj.get('$report_type') != 'TestReport':
                          continue
                      if obj.get('when') != 'call':
                          continue
                      if obj.get('outcome') == 'skipped':
                          continue
                      nodeid = obj.get('nodeid')
                      if nodeid:
                          keywords[nodeid] = obj.get('keywords') or {}
          except FileNotFoundError:
              print('pytest-report.jsonl not found; cannot verify docker markers.')
              sys.exit(0)

          offenders = sorted([n for n in used if 'docker' not in (keywords.get(n) or {})])
          with open('unmarked_docker_tests.txt', 'w') as out:
              for n in offenders:
                  out.write(f"{n}\n")
          if offenders:
              print('\nFound tests that used Docker but are missing @pytest.mark.docker:')
              for n in offenders:
                  print(f" - {n}")
              sys.exit(1)
          else:
              print('All Docker-using tests are correctly marked with @pytest.mark.docker.')
          PY

      - name: Upload timing reports
        if: always() && steps.check_commit.outputs.skip_tests != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: heavy-test-reports
          path: |
            pytest-report.jsonl
            unmarked_slow_tests.txt
            .docker_build_tests.txt
            .docker_compose_tests.txt
            unmarked_docker_tests.txt

      - name: Save tested inspect_evals SHA as artifact
        if: success() && github.event_name == 'schedule'
        run: |
          mkdir -p .last-inspect-evals-sha
          echo "${{ steps.inspect_commit.outputs.sha }}" > .last-inspect-evals-sha/last_sha.txt

      - name: Upload tested SHA artifact
        if: success() && github.event_name == 'schedule'
        uses: actions/upload-artifact@v4
        with:
          name: last-inspect-evals-sha
          path: .last-inspect-evals-sha/last_sha.txt

      - name: Cleanup Docker resources
        if: always()
        run: |
          docker system prune -f
          docker volume prune -f